{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Libraries\" data-toc-modified-id=\"Libraries-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Libraries</a></span></li><li><span><a href=\"#Data\" data-toc-modified-id=\"Data-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Data</a></span></li><li><span><a href=\"#Parse-the-data\" data-toc-modified-id=\"Parse-the-data-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Parse the data</a></span></li><li><span><a href=\"#Data-Exploration\" data-toc-modified-id=\"Data-Exploration-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Data Exploration</a></span></li><li><span><a href=\"#Pre-processing-Functions\" data-toc-modified-id=\"Pre-processing-Functions-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Pre-processing Functions</a></span><ul class=\"toc-item\"><li><span><a href=\"#Look-up-Table\" data-toc-modified-id=\"Look-up-Table-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Look-up Table</a></span></li><li><span><a href=\"#Tokenize-Punctuation\" data-toc-modified-id=\"Tokenize-Punctuation-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>Tokenize Punctuation</a></span></li><li><span><a href=\"#Apply-the-pre-processing-functions\" data-toc-modified-id=\"Apply-the-pre-processing-functions-5.3\"><span class=\"toc-item-num\">5.3&nbsp;&nbsp;</span>Apply the pre-processing functions</a></span></li></ul></li><li><span><a href=\"#Neural-Network\" data-toc-modified-id=\"Neural-Network-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Neural Network</a></span><ul class=\"toc-item\"><li><span><a href=\"#Input:-Batching\" data-toc-modified-id=\"Input:-Batching-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>Input: Batching</a></span></li><li><span><a href=\"#Neural-Network-Architecture\" data-toc-modified-id=\"Neural-Network-Architecture-6.2\"><span class=\"toc-item-num\">6.2&nbsp;&nbsp;</span>Neural Network Architecture</a></span></li><li><span><a href=\"#Forward-and-Backpropagation\" data-toc-modified-id=\"Forward-and-Backpropagation-6.3\"><span class=\"toc-item-num\">6.3&nbsp;&nbsp;</span>Forward and Backpropagation</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.nn as nn\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = './data/seinfeld-chronicles/scripts.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Character</th>\n",
       "      <th>Dialogue</th>\n",
       "      <th>EpisodeNo</th>\n",
       "      <th>SEID</th>\n",
       "      <th>Season</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>JERRY</td>\n",
       "      <td>Do you know what this is all about? Do you kno...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>S01E01</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>JERRY</td>\n",
       "      <td>(pointing at Georges shirt) See, to me, that b...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>S01E01</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>GEORGE</td>\n",
       "      <td>Are you through?</td>\n",
       "      <td>1.0</td>\n",
       "      <td>S01E01</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>JERRY</td>\n",
       "      <td>You do of course try on, when you buy?</td>\n",
       "      <td>1.0</td>\n",
       "      <td>S01E01</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>GEORGE</td>\n",
       "      <td>Yes, it was purple, I liked it, I dont actuall...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>S01E01</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0 Character                                           Dialogue  \\\n",
       "0           0     JERRY  Do you know what this is all about? Do you kno...   \n",
       "1           1     JERRY  (pointing at Georges shirt) See, to me, that b...   \n",
       "2           2    GEORGE                                   Are you through?   \n",
       "3           3     JERRY             You do of course try on, when you buy?   \n",
       "4           4    GEORGE  Yes, it was purple, I liked it, I dont actuall...   \n",
       "\n",
       "   EpisodeNo    SEID  Season  \n",
       "0        1.0  S01E01     1.0  \n",
       "1        1.0  S01E01     1.0  \n",
       "2        1.0  S01E01     1.0  \n",
       "3        1.0  S01E01     1.0  \n",
       "4        1.0  S01E01     1.0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scripts_df = pd.read_csv(data_dir)\n",
    "scripts_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse the data\n",
    "Desired format:\n",
    "<br>\n",
    "all one string, with the character name, colon, then lower-cased dialoge. \n",
    "<br>\n",
    "Example\n",
    "<br>\n",
    "'jerry:  do you know what this is all about? do you know, why were here?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to format text\n",
    "def scriptParser(character, dialogue):\n",
    "    if(isinstance(dialogue, str)):\n",
    "        dialogue = dialogue.lower()\n",
    "    else:\n",
    "        #dialogue = str(dialogue)\n",
    "        #print(dialogue) - always a nan\n",
    "        dialogue = ''\n",
    "    if(isinstance(character, str)):\n",
    "        character = character.lower()\n",
    "    else:\n",
    "        #character = str(character)\n",
    "        character = ''\n",
    "    \n",
    "    return character+': '+dialogue+'\\n\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'jerry: do you know what this is all about? do you know, why were here? to be out, this is out...and out is one of the single most enjoyable experiences of life. people...did you ever hear people talking about we should go out? this is what theyre talking about...this whole thing, were all out now, no one is home. not one person here is home, were all out! there are people tryin to find us, they dont know where we are. (on an imaginary phone) did you ring?, i cant find him. where did he go? he didnt tell me where he was going. he must have gone out. you wanna go out you get ready, you pick out the clothes, right? you take the shower, you get all ready, get the cash, get your friends, the car, the spot, the reservation...then youre standing around, whatta you do? you go we gotta be getting back. once youre out, you wanna get back! you wanna go to sleep, you wanna get up, you wanna go out again tomorrow, right? where ever you are in life, its my feeling, youve gotta go.\\n\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test the function on one row\n",
    "row = 0\n",
    "scriptParser(scripts_df['Character'].iloc[row], scripts_df['Dialogue'].iloc[row])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'jerry: do you know what this is all about? do you know, why were here? to be out, this is out...and out is one of the single most enjoyable experiences of life. people...did you ever hear people talking about we should go out? this is what theyre talking about...this whole thing, were all out now, no one is home. not one person here is home, were all out! there are people tryin to find us, they dont know where we are. (on an imaginary phone) did you ring?, i cant find him. where did he go? he didnt tell me where he was going. he must have gone out. you wanna go out you get ready, you pick out the clothes, right? you take the shower, you get all ready, get the cash, get your friends, the car, the spot, the reservation...then youre standing around, whatta you do? you go we gotta be getting back. once youre out, you wanna get back! you wanna go to sleep, you wanna get up, you wanna go out again tomorrow, right? where ever you are in life, its my feeling, youve gotta go.\\n\\njerry: (pointing at georges shirt) see, to me, that button is in the worst possible spot. the second button literally makes or breaks the shirt, look at it. its too high! its in no-mans-land. you look like you live with your mother.\\n\\ngeorge: are you through?\\n\\njerry: you do of course try on, when you buy?\\n\\ngeorge: yes, it was purple, i liked it, i dont actually recall considering the buttons.\\n\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test the function while joining on 5 rows\n",
    "''.join([scriptParser(row['Character'], row['Dialogue']) \n",
    "         for index, row in scripts_df[:5].iterrows()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the function\n",
    "text = ''.join([scriptParser(row['Character'], row['Dialogue']) \n",
    "         for index, row in scripts_df.iterrows()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Stats\n",
      "~ Number of unique words: 46380\n",
      "Number of lines: 109233\n",
      "Average number of words in each line: 5.544121282030155\n",
      "\n",
      "The lines 0 to 10:\n",
      "jerry: do you know what this is all about? do you know, why were here? to be out, this is out...and out is one of the single most enjoyable experiences of life. people...did you ever hear people talking about we should go out? this is what theyre talking about...this whole thing, were all out now, no one is home. not one person here is home, were all out! there are people tryin to find us, they dont know where we are. (on an imaginary phone) did you ring?, i cant find him. where did he go? he didnt tell me where he was going. he must have gone out. you wanna go out you get ready, you pick out the clothes, right? you take the shower, you get all ready, get the cash, get your friends, the car, the spot, the reservation...then youre standing around, whatta you do? you go we gotta be getting back. once youre out, you wanna get back! you wanna go to sleep, you wanna get up, you wanna go out again tomorrow, right? where ever you are in life, its my feeling, youve gotta go.\n",
      "\n",
      "jerry: (pointing at georges shirt) see, to me, that button is in the worst possible spot. the second button literally makes or breaks the shirt, look at it. its too high! its in no-mans-land. you look like you live with your mother.\n",
      "\n",
      "george: are you through?\n",
      "\n",
      "jerry: you do of course try on, when you buy?\n",
      "\n",
      "george: yes, it was purple, i liked it, i dont actually recall considering the buttons.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "view_line_range = (0, 10)\n",
    "\n",
    "print('Dataset Stats')\n",
    "print('~ Number of unique words: {}'.format(len({word: None for word in text.split()})))\n",
    "\n",
    "lines = text.split('\\n')\n",
    "print('Number of lines: {}'.format(len(lines)))\n",
    "word_count_line = [len(line.split()) for line in lines]\n",
    "print('Average number of words in each line: {}'.format(np.average(word_count_line)))\n",
    "\n",
    "print()\n",
    "print('The lines {} to {}:'.format(*view_line_range))\n",
    "print('\\n'.join(text.split('\\n')[view_line_range[0]:view_line_range[1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look-up Table\n",
    "2 dictionaries:\n",
    "<br>\n",
    "1. word to index: words2idx \n",
    "2. index to word: idx2word \n",
    "\n",
    "More common words should have a lower index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lookup_tables(text):\n",
    "    \"\"\"\n",
    "    Create lookup tables for vocabulary\n",
    "    \n",
    "    text: The text of tv scripts split into words\n",
    "    returns: A tuple of dicts (vocab_to_int, int_to_vocab)\n",
    "    \"\"\"\n",
    "    # first get the word_counts\n",
    "    word_counts = Counter(text)\n",
    "    \n",
    "    # sort from most to least frequent\n",
    "    sorted_vocab = sorted(word_counts, key=word_counts.get, reverse=True)\n",
    "    \n",
    "    # define the dictionaries\n",
    "    idx2word = {ii: word for ii, word in enumerate(sorted_vocab)}\n",
    "    word2idx = {word: ii for ii, word in idx2word.items()}\n",
    "    \n",
    "    # return tuple\n",
    "    return (idx2word, word2idx)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dict to turn punctuation into a token.\n",
    "punct2token = {'.': '<PERIOD>',\n",
    "                ',': '<COMMA>',\n",
    "                '\"': '<QUOTATION_MARK>',\n",
    "                ';': '<SEMICOLON>',\n",
    "                '!': '<EXCLAMATION_MARK>',\n",
    "                '?': '<QUESTION_MARK>',\n",
    "                '(': '<LEFT_PAREN>',\n",
    "                ')': '<RIGHT_PAREN>',\n",
    "                '--': ' <HYPHENS> ',\n",
    "                '-': '<DASH>',\n",
    "                '?': '<QUESTION_MARK>',\n",
    "                '\\n': '<NEW_LINE>',\n",
    "                ':': ' <COLON> '}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<COMMA>'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "punct2token[',']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply the pre-processing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the punctuation\n",
    "for punct, token in punct2token.items():\n",
    "    text = text.replace(punct, ' {} '.format(token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split and make all ensure all text is lower case\n",
    "text = text.lower()\n",
    "text = text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the vocab dictionaries\n",
    "idx2word, word2idx = create_lookup_tables(text + ['<PAD>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply dictionaries to text\n",
    "int_text = [word2idx[word] for word in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8,\n",
       " 2,\n",
       " 35,\n",
       " 5,\n",
       " 27,\n",
       " 19,\n",
       " 24,\n",
       " 23,\n",
       " 49,\n",
       " 58,\n",
       " 4,\n",
       " 35,\n",
       " 5,\n",
       " 27,\n",
       " 3,\n",
       " 80,\n",
       " 119,\n",
       " 61,\n",
       " 4,\n",
       " 9,\n",
       " 54,\n",
       " 47,\n",
       " 3,\n",
       " 24,\n",
       " 23,\n",
       " 47,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 18,\n",
       " 47,\n",
       " 23,\n",
       " 79,\n",
       " 21,\n",
       " 7,\n",
       " 1279,\n",
       " 547,\n",
       " 8340,\n",
       " 6825,\n",
       " 21,\n",
       " 240,\n",
       " 1,\n",
       " 147,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 81,\n",
       " 5,\n",
       " 198,\n",
       " 235,\n",
       " 147,\n",
       " 206,\n",
       " 58,\n",
       " 55,\n",
       " 133,\n",
       " 63,\n",
       " 47,\n",
       " 4,\n",
       " 24,\n",
       " 23,\n",
       " 19,\n",
       " 692,\n",
       " 206,\n",
       " 58,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 24,\n",
       " 218,\n",
       " 125,\n",
       " 3,\n",
       " 119,\n",
       " 49,\n",
       " 47,\n",
       " 83,\n",
       " 3,\n",
       " 26,\n",
       " 79,\n",
       " 23,\n",
       " 288,\n",
       " 1,\n",
       " 45,\n",
       " 79,\n",
       " 375,\n",
       " 61,\n",
       " 23,\n",
       " 288,\n",
       " 3,\n",
       " 119,\n",
       " 49,\n",
       " 47,\n",
       " 12,\n",
       " 75,\n",
       " 48,\n",
       " 147,\n",
       " 6826,\n",
       " 9,\n",
       " 247,\n",
       " 191,\n",
       " 3,\n",
       " 64,\n",
       " 202,\n",
       " 27,\n",
       " 127,\n",
       " 55,\n",
       " 48,\n",
       " 1,\n",
       " 13,\n",
       " 29,\n",
       " 95,\n",
       " 2681,\n",
       " 171,\n",
       " 14,\n",
       " 81,\n",
       " 5,\n",
       " 1403,\n",
       " 4,\n",
       " 3,\n",
       " 6,\n",
       " 388,\n",
       " 247,\n",
       " 68,\n",
       " 1,\n",
       " 127,\n",
       " 81,\n",
       " 36,\n",
       " 63,\n",
       " 4,\n",
       " 36,\n",
       " 530,\n",
       " 94,\n",
       " 25,\n",
       " 127,\n",
       " 36,\n",
       " 42,\n",
       " 78,\n",
       " 1,\n",
       " 36,\n",
       " 337,\n",
       " 40,\n",
       " 577,\n",
       " 47,\n",
       " 1,\n",
       " 5,\n",
       " 213,\n",
       " 63,\n",
       " 47,\n",
       " 5,\n",
       " 51,\n",
       " 426,\n",
       " 3,\n",
       " 5,\n",
       " 323,\n",
       " 47,\n",
       " 7,\n",
       " 561,\n",
       " 3,\n",
       " 57,\n",
       " 4,\n",
       " 5,\n",
       " 100,\n",
       " 7,\n",
       " 732,\n",
       " 3,\n",
       " 5,\n",
       " 51,\n",
       " 49,\n",
       " 426,\n",
       " 3,\n",
       " 51,\n",
       " 7,\n",
       " 1253,\n",
       " 3,\n",
       " 51,\n",
       " 52,\n",
       " 353,\n",
       " 3,\n",
       " 7,\n",
       " 158,\n",
       " 3,\n",
       " 7,\n",
       " 834,\n",
       " 3,\n",
       " 7,\n",
       " 2954,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 114,\n",
       " 313,\n",
       " 674,\n",
       " 173,\n",
       " 3,\n",
       " 4640,\n",
       " 5,\n",
       " 35,\n",
       " 4,\n",
       " 5,\n",
       " 63,\n",
       " 55,\n",
       " 138,\n",
       " 54,\n",
       " 188,\n",
       " 87,\n",
       " 1,\n",
       " 413,\n",
       " 313,\n",
       " 47,\n",
       " 3,\n",
       " 5,\n",
       " 213,\n",
       " 51,\n",
       " 87,\n",
       " 12,\n",
       " 5,\n",
       " 213,\n",
       " 63,\n",
       " 9,\n",
       " 509,\n",
       " 3,\n",
       " 5,\n",
       " 213,\n",
       " 51,\n",
       " 50,\n",
       " 3,\n",
       " 5,\n",
       " 213,\n",
       " 63,\n",
       " 47,\n",
       " 178,\n",
       " 382,\n",
       " 3,\n",
       " 57,\n",
       " 4,\n",
       " 127,\n",
       " 198,\n",
       " 5,\n",
       " 48,\n",
       " 22,\n",
       " 240,\n",
       " 3,\n",
       " 193,\n",
       " 33,\n",
       " 726,\n",
       " 3,\n",
       " 1567,\n",
       " 138,\n",
       " 63,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 8,\n",
       " 2,\n",
       " 13,\n",
       " 526,\n",
       " 59,\n",
       " 4641,\n",
       " 417,\n",
       " 14,\n",
       " 74,\n",
       " 3,\n",
       " 9,\n",
       " 25,\n",
       " 3,\n",
       " 20,\n",
       " 1041,\n",
       " 23,\n",
       " 22,\n",
       " 7,\n",
       " 945,\n",
       " 922,\n",
       " 834,\n",
       " 1,\n",
       " 7,\n",
       " 263,\n",
       " 1041,\n",
       " 3342,\n",
       " 376,\n",
       " 170,\n",
       " 1984,\n",
       " 7,\n",
       " 417,\n",
       " 3,\n",
       " 88,\n",
       " 59,\n",
       " 17,\n",
       " 1,\n",
       " 193,\n",
       " 132,\n",
       " 520,\n",
       " 12,\n",
       " 193,\n",
       " 22,\n",
       " 26,\n",
       " 34,\n",
       " 4217,\n",
       " 34,\n",
       " 1985,\n",
       " 1,\n",
       " 5,\n",
       " 88,\n",
       " 46,\n",
       " 5,\n",
       " 410,\n",
       " 41,\n",
       " 52,\n",
       " 411,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 11,\n",
       " 2,\n",
       " 48,\n",
       " 5,\n",
       " 275,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 8,\n",
       " 2,\n",
       " 5,\n",
       " 35,\n",
       " 21,\n",
       " 260,\n",
       " 380,\n",
       " 29,\n",
       " 3,\n",
       " 117,\n",
       " 5,\n",
       " 441,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 11,\n",
       " 2,\n",
       " 97,\n",
       " 3,\n",
       " 17,\n",
       " 42,\n",
       " 5163,\n",
       " 3,\n",
       " 6,\n",
       " 747,\n",
       " 17,\n",
       " 3,\n",
       " 6,\n",
       " 202,\n",
       " 315,\n",
       " 3139,\n",
       " 2834,\n",
       " 7,\n",
       " 2367,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 8,\n",
       " 2,\n",
       " 28,\n",
       " 3,\n",
       " 5,\n",
       " 202,\n",
       " 3139,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 11,\n",
       " 2,\n",
       " 13,\n",
       " 29,\n",
       " 95,\n",
       " 2681,\n",
       " 3140,\n",
       " 14,\n",
       " 71,\n",
       " 3,\n",
       " 26,\n",
       " 3,\n",
       " 45,\n",
       " 59,\n",
       " 24,\n",
       " 112,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 8,\n",
       " 2,\n",
       " 32,\n",
       " 3,\n",
       " 11392,\n",
       " 3,\n",
       " 1229,\n",
       " 44,\n",
       " 46,\n",
       " 9,\n",
       " 27,\n",
       " 3,\n",
       " 19,\n",
       " 5,\n",
       " 453,\n",
       " 18,\n",
       " 117,\n",
       " 5,\n",
       " 453,\n",
       " 17,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 2955,\n",
       " 2,\n",
       " 121,\n",
       " 1,\n",
       " 359,\n",
       " 1,\n",
       " 121,\n",
       " 1,\n",
       " 356,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 11,\n",
       " 2,\n",
       " 48,\n",
       " 3,\n",
       " 48,\n",
       " 5,\n",
       " 161,\n",
       " 24,\n",
       " 23,\n",
       " 3563,\n",
       " 4,\n",
       " 2682,\n",
       " 7,\n",
       " 2278,\n",
       " 11393,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 2955,\n",
       " 2,\n",
       " 193,\n",
       " 1280,\n",
       " 3,\n",
       " 6,\n",
       " 40,\n",
       " 9,\n",
       " 35,\n",
       " 17,\n",
       " 22,\n",
       " 33,\n",
       " 228,\n",
       " 3563,\n",
       " 277,\n",
       " 3,\n",
       " 1740,\n",
       " 57,\n",
       " 3,\n",
       " 3563,\n",
       " 277,\n",
       " 3,\n",
       " 1740,\n",
       " 57,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 193,\n",
       " 103,\n",
       " 5164,\n",
       " 208,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 8,\n",
       " 2,\n",
       " 77,\n",
       " 5,\n",
       " 1438,\n",
       " 3,\n",
       " 193,\n",
       " 10,\n",
       " 959,\n",
       " 21,\n",
       " 231,\n",
       " 1,\n",
       " 2955,\n",
       " 23,\n",
       " 10,\n",
       " 3564,\n",
       " 500,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 2955,\n",
       " 2,\n",
       " 1343,\n",
       " 25,\n",
       " 11,\n",
       " 1,\n",
       " 26,\n",
       " 79,\n",
       " 181,\n",
       " 160,\n",
       " 1854,\n",
       " 22,\n",
       " 556,\n",
       " 5,\n",
       " 29,\n",
       " 4218,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 11,\n",
       " 2,\n",
       " 72,\n",
       " 90,\n",
       " 313,\n",
       " 45,\n",
       " 2211,\n",
       " 7,\n",
       " 263,\n",
       " 209,\n",
       " 382,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 8,\n",
       " 2,\n",
       " 32,\n",
       " 3,\n",
       " 712,\n",
       " 24,\n",
       " 71,\n",
       " 3,\n",
       " 136,\n",
       " 350,\n",
       " 54,\n",
       " 4642,\n",
       " 22,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 11,\n",
       " 2,\n",
       " 164,\n",
       " 10,\n",
       " 263,\n",
       " 3,\n",
       " 164,\n",
       " 10,\n",
       " 263,\n",
       " 3,\n",
       " 19,\n",
       " 284,\n",
       " 22,\n",
       " 3,\n",
       " 19,\n",
       " 136,\n",
       " 23,\n",
       " 284,\n",
       " 22,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 8,\n",
       " 2,\n",
       " 6,\n",
       " 183,\n",
       " 5,\n",
       " 58,\n",
       " 1115,\n",
       " 3,\n",
       " 7,\n",
       " 548,\n",
       " 6,\n",
       " 476,\n",
       " 22,\n",
       " 3565,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 11,\n",
       " 2,\n",
       " 26,\n",
       " 3,\n",
       " 5,\n",
       " 530,\n",
       " 12,\n",
       " 0,\n",
       " 0,\n",
       " 8,\n",
       " 2,\n",
       " 6,\n",
       " 194,\n",
       " 6,\n",
       " 183,\n",
       " 5,\n",
       " 58,\n",
       " 17,\n",
       " 3,\n",
       " 97,\n",
       " 3,\n",
       " 67,\n",
       " 11394,\n",
       " 8341,\n",
       " 2683,\n",
       " 4,\n",
       " 6,\n",
       " 476,\n",
       " 62,\n",
       " 7,\n",
       " 207,\n",
       " 6,\n",
       " 81,\n",
       " 7,\n",
       " 209,\n",
       " 22,\n",
       " 11395,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 11,\n",
       " 2,\n",
       " 195,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 8,\n",
       " 2,\n",
       " 13,\n",
       " 214,\n",
       " 22,\n",
       " 7,\n",
       " 11396,\n",
       " 14,\n",
       " 712,\n",
       " 26,\n",
       " 1377,\n",
       " 22,\n",
       " 61,\n",
       " 3,\n",
       " 19,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 11,\n",
       " 2,\n",
       " 164,\n",
       " 164,\n",
       " 164,\n",
       " 3,\n",
       " 19,\n",
       " 23,\n",
       " 67,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 13,\n",
       " 347,\n",
       " 7,\n",
       " 1377,\n",
       " 77,\n",
       " 92,\n",
       " 8,\n",
       " 18,\n",
       " 569,\n",
       " 17,\n",
       " 29,\n",
       " 7,\n",
       " 300,\n",
       " 14,\n",
       " 19,\n",
       " 23,\n",
       " 67,\n",
       " 46,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 8,\n",
       " 2,\n",
       " 28,\n",
       " 3,\n",
       " 693,\n",
       " 82,\n",
       " 174,\n",
       " 1,\n",
       " 6,\n",
       " 116,\n",
       " 3,\n",
       " 693,\n",
       " 60,\n",
       " 46,\n",
       " 10,\n",
       " 338,\n",
       " 8342,\n",
       " 58,\n",
       " 62,\n",
       " 18,\n",
       " 693,\n",
       " 82,\n",
       " 2956,\n",
       " 18,\n",
       " 82,\n",
       " 309,\n",
       " 18,\n",
       " 71,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 7,\n",
       " 814,\n",
       " 793,\n",
       " 3,\n",
       " 6,\n",
       " 116,\n",
       " 3,\n",
       " 17,\n",
       " 42,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 206,\n",
       " 41,\n",
       " 62,\n",
       " 23,\n",
       " 46,\n",
       " 206,\n",
       " 41,\n",
       " 5,\n",
       " 3,\n",
       " 73,\n",
       " 3,\n",
       " 5,\n",
       " 27,\n",
       " 3,\n",
       " 780,\n",
       " 175,\n",
       " 219,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 11,\n",
       " 2,\n",
       " 13,\n",
       " 684,\n",
       " 14,\n",
       " 38,\n",
       " 3,\n",
       " 5,\n",
       " 27,\n",
       " 3,\n",
       " 19,\n",
       " 3,\n",
       " 19,\n",
       " 204,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 8,\n",
       " 2,\n",
       " 28,\n",
       " 3,\n",
       " 184,\n",
       " 204,\n",
       " 3,\n",
       " 5,\n",
       " 27,\n",
       " 3,\n",
       " 73,\n",
       " 23,\n",
       " 42,\n",
       " 174,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 11,\n",
       " 2,\n",
       " 28,\n",
       " 3,\n",
       " 184,\n",
       " 204,\n",
       " 3,\n",
       " 73,\n",
       " 17,\n",
       " 42,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 8,\n",
       " 2,\n",
       " 30,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 11,\n",
       " 2,\n",
       " 24,\n",
       " 23,\n",
       " 174,\n",
       " 12,\n",
       " 0,\n",
       " 0,\n",
       " 8,\n",
       " 2,\n",
       " 30,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 11,\n",
       " 2,\n",
       " 38,\n",
       " 3,\n",
       " 5,\n",
       " 27,\n",
       " 3,\n",
       " 67,\n",
       " 835,\n",
       " 18,\n",
       " 357,\n",
       " 67,\n",
       " 369,\n",
       " 9,\n",
       " 63,\n",
       " 47,\n",
       " 41,\n",
       " 5,\n",
       " 382,\n",
       " 207,\n",
       " 4,\n",
       " 217,\n",
       " 1916,\n",
       " 12,\n",
       " 3859,\n",
       " 5,\n",
       " 12,\n",
       " 0,\n",
       " 0,\n",
       " 8,\n",
       " 2,\n",
       " 30,\n",
       " 3,\n",
       " 32,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 45,\n",
       " 611,\n",
       " 1,\n",
       " 6,\n",
       " 116,\n",
       " 3,\n",
       " 67,\n",
       " 143,\n",
       " 3,\n",
       " 5,\n",
       " 27,\n",
       " 3,\n",
       " 67,\n",
       " 341,\n",
       " 24,\n",
       " 427,\n",
       " 18,\n",
       " 143,\n",
       " 67,\n",
       " 102,\n",
       " 9,\n",
       " 90,\n",
       " 22,\n",
       " 39,\n",
       " 10,\n",
       " 6827,\n",
       " 18,\n",
       " 146,\n",
       " 32,\n",
       " 51,\n",
       " 318,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 11,\n",
       " 2,\n",
       " 13,\n",
       " 4643,\n",
       " 6828,\n",
       " 14,\n",
       " 585,\n",
       " 585,\n",
       " 585,\n",
       " 3,\n",
       " 102,\n",
       " 9,\n",
       " 4,\n",
       " 102,\n",
       " 9,\n",
       " 90,\n",
       " 22,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 8,\n",
       " 2,\n",
       " 30,\n",
       " 3,\n",
       " 73,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 11,\n",
       " 2,\n",
       " 102,\n",
       " 9,\n",
       " 90,\n",
       " 22,\n",
       " 18,\n",
       " 146,\n",
       " 32,\n",
       " 51,\n",
       " 318,\n",
       " 4,\n",
       " 102,\n",
       " 9,\n",
       " 18,\n",
       " 146,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 8,\n",
       " 2,\n",
       " 30,\n",
       " 12,\n",
       " 0,\n",
       " 0,\n",
       " 11,\n",
       " 2,\n",
       " 26,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 26,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 26,\n",
       " 3,\n",
       " 6,\n",
       " 482,\n",
       " 9,\n",
       " 94,\n",
       " 5,\n",
       " 24,\n",
       " 1,\n",
       " 313,\n",
       " 45,\n",
       " 85,\n",
       " 74,\n",
       " 24,\n",
       " 136,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 8,\n",
       " 2,\n",
       " 19,\n",
       " 3,\n",
       " 48,\n",
       " 5,\n",
       " 894,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 80,\n",
       " 3,\n",
       " 80,\n",
       " 81,\n",
       " 67,\n",
       " 152,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 11,\n",
       " 2,\n",
       " 72,\n",
       " 35,\n",
       " 6,\n",
       " 27,\n",
       " 3,\n",
       " 146,\n",
       " 3,\n",
       " 5,\n",
       " 27,\n",
       " 3,\n",
       " 146,\n",
       " 67,\n",
       " 312,\n",
       " ...]"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to a pickle file\n",
    "pickle.dump((int_text, idx2word, word2idx, punct2token), open('preprocess.p', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU found. Please use a GPU to train your neural network.\n"
     ]
    }
   ],
   "source": [
    "# Check GPU Access\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "if not train_on_gpu:\n",
    "    print('No GPU found. Please use a GPU to train your neural network.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input: Batching\n",
    "We want to batch the data based on a provided sequence legnth\n",
    "Example Input:\n",
    "```\n",
    "words = [1, 2, 3, 4, 5, 6, 7]\n",
    "sequence_length = 4\n",
    "```\n",
    "\n",
    "`features` Tensor:\n",
    "```\n",
    "[1, 2, 3, 4]\n",
    "```\n",
    "`target` Tensor (the next \"word\"):\n",
    "```\n",
    "5\n",
    "```\n",
    "\n",
    "Next `features` and `target` Tensors:\n",
    "```\n",
    "[2, 3, 4, 5]  # features\n",
    "6             # target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_data(words, sequence_length, batch_size):\n",
    "    \"\"\"\n",
    "    Batch the neural network data using DataLoader\n",
    "    param words: The word ids of the TV scripts\n",
    "    param sequence_length: The sequence length of each batch\n",
    "    param batch_size: The size of each batch; the number of sequences in a batch\n",
    "    return: DataLoader with batched data\n",
    "    \"\"\"\n",
    "    \n",
    "    # iterate overall all words\n",
    "    features = []\n",
    "    targets = []\n",
    "    for i, w in enumerate(words):\n",
    "        if((i+sequence_length) < len(words)):\n",
    "            features.append(words[i:(i+sequence_length)])\n",
    "            targets.append(words[i+sequence_length])\n",
    "    \n",
    "    features = np.array(features)\n",
    "    targets = np.array(targets)\n",
    "    \n",
    "    # convert numpy arrays into a torch tensor dataset\n",
    "    sequenced_data = TensorDataset(torch.from_numpy(features), torch.from_numpy(targets))\n",
    "    \n",
    "    # create the final batch DataLoader\n",
    "    batch_dL = DataLoader(sequenced_data, shuffle=True, batch_size=batch_size) \n",
    "\n",
    "    # return a dataloader\n",
    "    return batch_dL\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 5])\n",
      "tensor([[42, 43, 44, 45, 46],\n",
      "        [ 8,  9, 10, 11, 12],\n",
      "        [10, 11, 12, 13, 14],\n",
      "        [28, 29, 30, 31, 32],\n",
      "        [41, 42, 43, 44, 45],\n",
      "        [24, 25, 26, 27, 28],\n",
      "        [ 9, 10, 11, 12, 13],\n",
      "        [44, 45, 46, 47, 48],\n",
      "        [35, 36, 37, 38, 39],\n",
      "        [20, 21, 22, 23, 24]], dtype=torch.int32)\n",
      "\n",
      "torch.Size([10])\n",
      "tensor([47, 13, 15, 33, 46, 29, 14, 49, 40, 25], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "# Test the Data Loader\n",
    "\n",
    "test_text = range(50)\n",
    "t_loader = batch_data(test_text, sequence_length=5, batch_size=10)\n",
    "\n",
    "data_iter = iter(t_loader)\n",
    "sample_x, sample_y = data_iter.next()\n",
    "\n",
    "print(sample_x.shape)\n",
    "print(sample_x)\n",
    "print()\n",
    "print(sample_y.shape)\n",
    "print(sample_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Architecture\n",
    "Define an LSTM-based RNN class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, dropout=0.5):\n",
    "        \"\"\"\n",
    "        Initialize the PyTorch RNN Module\n",
    "        :param vocab_size: The number of input dimensions of the neural network (the size of the vocabulary)\n",
    "        :param output_size: The number of output dimensions of the neural network\n",
    "        :param embedding_dim: The size of embeddings, should you choose to use them        \n",
    "        :param hidden_dim: The size of the hidden layer outputs\n",
    "        :param dropout: dropout to add in between LSTM/GRU layers\n",
    "        \"\"\"\n",
    "        super(RNN, self).__init__()\n",
    "       \n",
    "         # set class variables\n",
    "        self.vocab_size = vocab_size\n",
    "        self.output_size = output_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.drop_prob = dropout\n",
    "        self.batch_size = None\n",
    "        \n",
    "       \n",
    "        # define model layers\n",
    "        \n",
    "        # embedding layer \n",
    "        self.embed = nn.Embedding(self.vocab_size, self.embedding_dim)\n",
    "        \n",
    "        # LSTM layer\n",
    "        self.lstm = nn.LSTM(self.embedding_dim, self.hidden_dim, self.n_layers, dropout=self.drop_prob, batch_first=True)\n",
    "        \n",
    "        # dropout layer\n",
    "        self.dropout = nn.Dropout(self.drop_prob)\n",
    "        \n",
    "        # fully-connected layer\n",
    "        self.fc = nn.Linear(self.hidden_dim, self.output_size)\n",
    "    \n",
    "    \n",
    "    def forward(self, nn_input, hidden):\n",
    "        \"\"\"\n",
    "        Forward propagation of the neural network\n",
    "        :param nn_input: The input to the neural network\n",
    "        :param hidden: The hidden state        \n",
    "        :return: Two Tensors, the output of the neural network and the latest hidden state\n",
    "        \"\"\"\n",
    "        # embeddings\n",
    "        x = self.embed(nn_input)\n",
    "        \n",
    "        # get the LSTM outputs\n",
    "        l_out, hidden = self.lstm(x, hidden)\n",
    "        \n",
    "        # stack up the LSTM outputs - aka shape the output\n",
    "        l_out = l_out.contiguous().view(-1, self.hidden_dim)\n",
    "        \n",
    "        # dropout\n",
    "        out = self.dropout(l_out)\n",
    "        \n",
    "        # get the final output\n",
    "        out = self.fc(out)\n",
    "\n",
    "        # reshape into (batch_size, seq_length, output_size)\n",
    "        out = out.view(self.batch_size, -1, self.output_size)\n",
    "\n",
    "        # get last batch\n",
    "        out = out[:, -1]\n",
    "        \n",
    "        # return one batch of output word scores and the hidden state\n",
    "        return out, hidden\n",
    "    \n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        '''\n",
    "        Initialize the hidden state of an LSTM/GRU\n",
    "        :param batch_size: The batch_size of the hidden state\n",
    "        :return: hidden state of dims (n_layers, batch_size, hidden_dim)\n",
    "        '''\n",
    "        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        # and move to GPU if available\n",
    "        \n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        if(train_on_gpu):\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),\n",
    "                     weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n",
    "                     weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n",
    "        \n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward and Backpropagation\n",
    "Define a function \n",
    "```\n",
    "forward_back_prop()\n",
    "```\n",
    "That will be used in training to find the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_back_prop(rnn, optimizer, criterion, inp, target, hidden):\n",
    "    \"\"\"\n",
    "    Forward and backward propagation on the neural network\n",
    "    param decoder: The PyTorch Module that holds the neural network\n",
    "    param decoder_optimizer: The PyTorch optimizer for the neural network\n",
    "    param criterion: The PyTorch loss function\n",
    "    param inp: A batch of input to the neural network\n",
    "    param target: The target output for the batch of input\n",
    "    return: The loss and the latest hidden state Tensor\n",
    "    \"\"\"\n",
    "    \n",
    "    # move data to GPU, if available\n",
    "    if(train_on_gpu):\n",
    "        inp, target = inp.cuda(), target.cuda()\n",
    "        \n",
    "    # create new varaibles for the hidden state\n",
    "    hidden = tuple([each.data for each in hidden])\n",
    "    \n",
    "    # zero out any accumulated gradients\n",
    "    rnn.zero_grad()\n",
    "    \n",
    "    # get the model outputs\n",
    "    output, hidden = rnn(inp, hidden)\n",
    "    \n",
    "    # calculate the loss & perform backprop\n",
    "    loss = criterion(output.squeeze(), target)\n",
    "    loss.backward()\n",
    "    \n",
    "    # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "    # nn.utils.clip_grad_norm_(rnn.parameters(), clip)\n",
    "    # skip clipping for now\n",
    "    \n",
    "    # perform an optimization step\n",
    "    optimizer.step()\n",
    "\n",
    "    # return the loss over a batch and the hidden state produced by our model\n",
    "    return loss.item(), hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "331px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
